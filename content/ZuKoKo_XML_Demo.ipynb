{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e33774d",
   "metadata": {},
   "source": [
    "\n",
    "# XML für die Korpuslinguistik — Workshop (90 Minuten) an der ZuKoKo\n",
    "\n",
    "**Dozent**: Phillip B. Ströbel (UZH)  \n",
    "**Ziele**: In 90 Minuten weisst du, …  \n",
    "- was XML ist und warum es sich in der Korpuslinguistik bewährt,  \n",
    "- was Header/Body bedeutet (z. B. TEI),  \n",
    "- wie Tags/Attribute/Hierarchien funktionieren,  \n",
    "- was der Unterschied zwischen flachem und tiefem XML ist,  \n",
    "- welche Python-Bibliotheken sich für XML eignen und wie man einfache Analysen macht.\n",
    "\n",
    "**Material**: Dieses Notebook findest du unter https://pstroe.github.io/xml-workshop-zukoko-2025/lab/index.html. Die Daten im Ordner `data/`.\n",
    "\n",
    "Dieses Notebook verwendet **`SAC-Jahrbuch_1899_mul.tagged.xml`** als durchgehendes Beispiel.\n",
    "Wir demonstrieren:\n",
    "- Parsen & Traversieren (Elemente, Attribute, Hierarchie)\n",
    "- Abfragen (XPath-ähnliche Pfade, optional `lxml`)\n",
    "- Token-/POS-/Lemma-Analysen\n",
    "- Type-Token-Ratio pro Artikel\n",
    "- Konkordanzen & Kontext (KWIC)\n",
    "- Export als CSV\n",
    "- (Optional) einfache Visualisierung\n",
    "\n",
    "> Hinweis: Plots nutzen `matplotlib` (keine Styles/Farben gesetzt).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580410e9",
   "metadata": {},
   "source": [
    "## 0) Setup & Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69466422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "SAC_PATH = Path(r'/data/SAC-Jahrbuch_1899_mul.tagged.xml\")\n",
    "print(\"Datei:\", SAC_PATH.name, \"Größe:\", SAC_PATH.stat().st_size, \"Bytes\")\n",
    "\n",
    "# Optional: lxml für komfortablere XPath\n",
    "try:\n",
    "    import lxml.etree as LET\n",
    "    USING_LXML = True\n",
    "except Exception:\n",
    "    USING_LXML = False\n",
    "\n",
    "USING_LXML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec793d1",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Parsen & erste Inspektion\n",
    "Wir lesen das XML als Baum und schauen uns die oberste Struktur an.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5638f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tree = ET.parse(SAC_PATH)\n",
    "root = tree.getroot()\n",
    "print(\"Root-Tag:\", root.tag)\n",
    "children = list(root)\n",
    "print(\"Erste Kinder:\", [c.tag for c in children[:5]])\n",
    "articles = root.findall(\".//article\")\n",
    "print(\"Anzahl Artikel:\", len(articles))\n",
    "if articles:\n",
    "    first_article = articles[0]\n",
    "    first_sents = first_article.findall(\".//s\")[:3]\n",
    "    print(\"Beispiel: #Sätze im 1. Artikel (erste 3 gezeigt):\", len(first_sents))\n",
    "    if first_sents:\n",
    "        print(\"Attr. des ersten Satzes:\", first_sents[0].attrib)\n",
    "        print(\"Erste 10 Wort-Tags des ersten Satzes:\", [w.tag for w in first_sents[0].findall('.//w')[:10]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b12b63c",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Elemente, Attribute, Elementinhalt\n",
    "Wir extrahieren bei `<w>`:\n",
    "- **Elementinhalt** (Text)\n",
    "- **Attribute**: `lemma`, `pos`\n",
    "\n",
    "10 Beispiele:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dbd920",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "examples = []\n",
    "for w in root.findall(\".//w\"):\n",
    "    examples.append({\n",
    "        \"form (Elementinhalt)\": (w.text or \"\"),\n",
    "        \"lemma\": w.attrib.get(\"lemma\"),\n",
    "        \"pos\": w.attrib.get(\"pos\")\n",
    "    })\n",
    "    if len(examples) >= 10:\n",
    "        break\n",
    "examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d1481",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Artikel-IDs, Sätze & Wörter zählen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a0f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def iter_articles(root):\n",
    "    for art in root.findall(\".//article\"):\n",
    "        yield art\n",
    "\n",
    "def article_id(art):\n",
    "    return art.attrib.get(\"n\")\n",
    "\n",
    "def count_sent_words(art):\n",
    "    sents = art.findall(\".//s\")\n",
    "    words = art.findall(\".//w\")\n",
    "    return len(sents), len(words)\n",
    "\n",
    "summary = []\n",
    "for art in iter_articles(root):\n",
    "    sid, wid = count_sent_words(art)\n",
    "    summary.append({\"article_n\": article_id(art), \"sentences\": sid, \"words\": wid})\n",
    "\n",
    "import pandas as pd\n",
    "df_summary = pd.DataFrame(summary).sort_values(\"article_n\", key=lambda s: s.astype(str))\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7bd94d",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Token-/Lemma-/POS-Tabellen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddbd613",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = []\n",
    "for art in iter_articles(root):\n",
    "    a_id = article_id(art)\n",
    "    for s in art.findall(\".//s\"):\n",
    "        s_n = s.attrib.get(\"n\")\n",
    "        for w in s.findall(\".//w\"):\n",
    "            rows.append({\n",
    "                \"article_n\": a_id,\n",
    "                \"sent_n\": s_n,\n",
    "                \"form\": w.text or \"\",\n",
    "                \"lemma\": w.attrib.get(\"lemma\"),\n",
    "                \"pos\": w.attrib.get(\"pos\")\n",
    "            })\n",
    "\n",
    "import pandas as pd\n",
    "df_tokens = pd.DataFrame(rows)\n",
    "df_tokens.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04bf2ed",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Häufigkeiten (Formen, Lemmata, POS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cd6172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "form_freq = df_tokens[\"form\"].value_counts().reset_index()\n",
    "form_freq.columns = [\"form\", \"freq\"]\n",
    "lemma_freq = df_tokens[\"lemma\"].value_counts().reset_index()\n",
    "lemma_freq.columns = [\"lemma\", \"freq\"]\n",
    "pos_freq = df_tokens[\"pos\"].value_counts().reset_index()\n",
    "pos_freq.columns = [\"pos\", \"freq\"]\n",
    "\n",
    "form_freq.head(20), lemma_freq.head(20), pos_freq.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff53c7b0",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Type-Token-Ratio (TTR) pro Artikel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb6772",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ttr(series):\n",
    "    tokens = series.dropna().tolist()\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "    types = set(tokens)\n",
    "    return len(types) / len(tokens)\n",
    "\n",
    "ttr_by_article = (\n",
    "    df_tokens.groupby(\"article_n\")[\"form\"]\n",
    "    .apply(ttr)\n",
    "    .reset_index(name=\"TTR_form\")\n",
    "    .merge(\n",
    "        df_tokens.groupby(\"article_n\")[\"lemma\"].apply(ttr).reset_index(name=\"TTR_lemma\"),\n",
    "        on=\"article_n\", how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "ttr_by_article\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3433e8",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Konkordanz / KWIC (Key Word in Context)\n",
    "Einfacher KWIC für ein gegebenes **Lemma** oder eine **Form**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b44ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TARGET = \"Jahrbuch\"  # ändere auf ein Lemma oder eine Form\n",
    "WINDOW = 5\n",
    "\n",
    "def sent_tokens(s):\n",
    "    return [w.text or \"\" for w in s.findall(\".//w\")]\n",
    "\n",
    "def kwic_for_form(root, form, window=5, limit=20):\n",
    "    out = []\n",
    "    for s in root.findall(\".//s\"):\n",
    "        toks = sent_tokens(s)\n",
    "        for i, tok in enumerate(toks):\n",
    "            if tok == form:\n",
    "                left = \" \".join(toks[max(0, i-window):i])\n",
    "                right = \" \".join(toks[i+1:i+1+window])\n",
    "                out.append({\"left\": left, \"kw\": tok, \"right\": right, \"sent_n\": s.attrib.get(\"n\")})\n",
    "                if len(out) >= limit:\n",
    "                    return out\n",
    "    return out\n",
    "\n",
    "kwic_for_form(root, TARGET, window=WINDOW, limit=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e252bc6",
   "metadata": {},
   "source": [
    "\n",
    "## 8) (Optional) XPath mit lxml\n",
    "Wenn `lxml` verfügbar ist, zeigen wir ein XPath-Beispiel (alle Wörter mit `pos=\"NN\"` im ersten Artikel).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb53ab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if USING_LXML:\n",
    "    ltree = LET.parse(str(SAC_PATH))\n",
    "    nodes = ltree.xpath(\"//article[1]//w[@pos='NN']\")\n",
    "    [n.text for n in nodes[:20]]\n",
    "else:\n",
    "    print(\"lxml nicht verfügbar — Abschnitt übersprungen.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0365f6",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Visualisierung (einfach): POS-Verteilung insgesamt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ced04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pos_counts = df_tokens[\"pos\"].value_counts().reset_index()\n",
    "pos_counts.columns = [\"pos\", \"freq\"]\n",
    "plt.figure()\n",
    "plt.bar(pos_counts[\"pos\"], pos_counts[\"freq\"])  # keine Farben explizit setzen\n",
    "plt.title(\"POS-Verteilung (gesamt)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19ac19",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Export: Tokens als CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03a244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_csv = SAC_PATH.with_name(\"SAC_tokens.csv\")\n",
    "df_tokens.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "out_csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdca3aee-e256-4fc2-be74-b074cff8ea1f",
   "metadata": {},
   "source": [
    "## 11) Validierung gegen Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb83ca0-3e86-4318-a8fc-cdb5da89f8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfad zur Schema-Datei (im gleichen Ordner wie die XML-Datei)\n",
    "xsd_path = SAC_PATH.with_name(\"corpus.xsd\")\n",
    "\n",
    "# Lade XML und Schema\n",
    "xml_doc = etree.parse(str(SAC_PATH))\n",
    "with open(xsd_path, \"rb\") as f:\n",
    "    xmlschema_doc = etree.parse(f)\n",
    "xmlschema = etree.XMLSchema(xmlschema_doc)\n",
    "\n",
    "# Validierung\n",
    "is_valid = xmlschema.validate(xml_doc)\n",
    "print(\"Validierungsergebnis:\", is_valid)\n",
    "\n",
    "# Falls Fehler auftreten, diese anzeigen\n",
    "if not is_valid:\n",
    "    for error in xmlschema.error_log:\n",
    "        print(error.message, \"→ Zeile\", error.line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9729ec1",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Aufgaben (für die Teilnehmenden)\n",
    "1. **POS je Artikel**: Erstelle eine Tabelle `article_n × pos` (Absolute / relative Häufigkeiten).\n",
    "2. **Lemmata-Topliste**: Finde die häufigsten Lemmata pro Artikel (Top 10).\n",
    "3. **Satzlängen**: Plot der Verteilung der Satzlängen (Anzahl Tokens pro Satz).\n",
    "4. **Regex-Filter**: Finde alle Formen, die mit Großbuchstaben beginnen (Eigennamen-Heuristik).\n",
    "5. **Export**: Speichere Ergebnisse als CSV.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
